{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 11에서 발생하는 버그 수정\n",
    "import inspect\n",
    "\n",
    "if not hasattr(inspect, 'getargspec'):\n",
    "    inspect.getargspec = inspect.getfullargspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(paths.weights_file))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransPoseNet(\n",
       "  (pose_s1): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=72, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=512, out_features=15, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pose_s2): RNN(\n",
       "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=87, out_features=64, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=69, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pose_s3): RNN(\n",
       "    (rnn): LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=141, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=90, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tran_b1): RNN(\n",
       "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=87, out_features=64, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tran_b2): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "    (linear1): Linear(in_features=141, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net import TransPoseNet\n",
    "import torch\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = TransPoseNet()\n",
    "model.eval()  # 반드시 eval 모드로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:176: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  lfoot_pos, rfoot_pos = art.math.forward_kinematics(pose[joint_set.lower_body].unsqueeze(0),\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:181: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  indices = torch.tensor([7, 8])  # 7:9에 해당하는 인덱스\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:190: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  decision = (contact_probability[0] > contact_probability[1]).item()\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:201: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if current_foot_y + velocity[1].item() <= self.floor_y:\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:201: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if current_foot_y + velocity[1].item() <= self.floor_y:\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\net.py:203: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.current_root_y += velocity[1].item()\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\articulate\\math\\angular.py:163: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  result = torch.stack([\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\articulate\\math\\angular.py:164: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  torch.tensor(cv2.Rodrigues(_.detach().cpu().numpy())[0])  # NumPy로 변환 후 처리\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\articulate\\math\\angular.py:164: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(cv2.Rodrigues(_.detach().cpu().numpy())[0])  # NumPy로 변환 후 처리\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\numpy\\core\\shape_base.py:212: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  return tuple(arrays)\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\numpy\\core\\shape_base.py:212: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  return tuple(arrays)\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\numpy\\core\\shape_base.py:443: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  arrays = [asanyarray(arr) for arr in arrays]\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\articulate\\math\\angular.py:169: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  result = torch.from_numpy(np.stack(result)).float().squeeze(-1).to(r.device)\n",
      "c:\\Users\\sage\\SynologyDrive\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 모델이 transpose_net_241217.onnx에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 더미 입력 생성 (예: [batch_size, input_size])\n",
    "# TransPoseNet에서 필요한 IMU 입력 크기를 확인하여 더미 입력 생성\n",
    "# dummy_input = torch.randn(1, 6 * 3 + 6 * 9)  # 예시: (1, IMU 크기)\n",
    "dummy_input = (torch.randn(1, 6 * 3), torch.randn(1, 6 * 9))  # 예시: (1, IMU 크기)\n",
    "\n",
    "# ONNX로 변환\n",
    "onnx_file_path = \"transpose_net_241217.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,  # PyTorch 모델\n",
    "    dummy_input,  # 예제 입력\n",
    "    onnx_file_path,  # 저장할 ONNX 파일 경로\n",
    "    export_params=True,  # 가중치를 함께 저장\n",
    "    opset_version=11,  # ONNX Opset 버전 (최소 11 이상 권장)\n",
    "    input_names=[\"acc\", \"ori\"],  # 입력 이름\n",
    "    output_names=[\"pose\", \"tran\"],  # 출력 이름\n",
    "    dynamic_axes={\n",
    "        \"acc\": {0: \"batch_size\"},  # 배치 크기 동적 지원\n",
    "        \"ori\": {0: \"batch_size\"},\n",
    "        \"pose\": {0: \"batch_size\"},\n",
    "        \"tran\": {0: \"batch_size\"},\n",
    "    }\n",
    ")\n",
    "print(f\"ONNX 모델이 {onnx_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX 경량화 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxsim import simplify\n",
    "import onnx\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model = onnx.load(\"transpose_net_241217.onnx\")\n",
    "\n",
    "# Simplify 모델\n",
    "# simplified_model, check = simplify(onnx_model)\n",
    "\n",
    "# Simplify 모델, 검증 비활성화\n",
    "simplified_model, check = simplify(onnx_model, skip_fuse_bn=True, check_n=False)\n",
    "\n",
    "# 단순화 후 저장\n",
    "onnx.save(simplified_model, \"simplified_model_241217.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch 모델 (실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nundefined value torch:\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 227\n        :return: Pose tensor in shape [24, 3, 3] and translation tensor in shape [3].\n        \"\"\"\n        imu = x.repeat(self.num_total_frame, 1) if self.imu is None else torch.cat((self.imu[1:], x.view(1, -1)))\n                                                                         ~~~~~ <--- HERE\n        \n        if self.rnn_state is not None:\n'TransPoseNet.forward_online' is being compiled since it was called from 'TransPoseNet.forward'\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py\", line 169\n        data_nn = torch.cat((acc.flatten(1), ori.flatten(1)), dim=1)\n    \n        pose, tran = self.forward_online(data_nn)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        pose = rotation_matrix_to_axis_angle(pose.view(1, 216)).view(72)\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dummy Input: IMU 데이터 (예: [batch_size, n_imu])\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# dummy_input = torch.randn(1, 69)  # Replace 69 with your model's input size\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# TorchScript 변환\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m scripted_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m      8\u001b[0m scripted_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose_net.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1429\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1428\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1438\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1147\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1146\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:557\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    556\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:634\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:466\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    463\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m    464\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 466\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:1035\u001b[0m, in \u001b[0;36mcompile_unbound_method\u001b[1;34m(concrete_type, fn)\u001b[0m\n\u001b[0;32m   1031\u001b[0m stub \u001b[38;5;241m=\u001b[39m make_stub(fn, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39m_disable_emit_hooks():\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# We don't want to call the hooks here since the graph that is calling\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;66;03m# this function is not yet complete\u001b[39;00m\n\u001b[1;32m-> 1035\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stub\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:466\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    463\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m    464\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 466\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \nundefined value torch:\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 227\n        :return: Pose tensor in shape [24, 3, 3] and translation tensor in shape [3].\n        \"\"\"\n        imu = x.repeat(self.num_total_frame, 1) if self.imu is None else torch.cat((self.imu[1:], x.view(1, -1)))\n                                                                         ~~~~~ <--- HERE\n        \n        if self.rnn_state is not None:\n'TransPoseNet.forward_online' is being compiled since it was called from 'TransPoseNet.forward'\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py\", line 169\n        data_nn = torch.cat((acc.flatten(1), ori.flatten(1)), dim=1)\n    \n        pose, tran = self.forward_online(data_nn)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        pose = rotation_matrix_to_axis_angle(pose.view(1, 216)).view(72)\n    \n"
     ]
    }
   ],
   "source": [
    "# Dummy Input: IMU 데이터 (예: [batch_size, n_imu])\n",
    "# dummy_input = torch.randn(1, 69)  # Replace 69 with your model's input size\n",
    "\n",
    "# TorchScript 변환\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# 모델 저장\n",
    "scripted_model.save(\"transpose_net.pt\")\n",
    "print(\"TorchScript 모델이 'transpose_net.pt'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "quantized_model = quantize_dynamic(model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"quantized_transpose_net.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
