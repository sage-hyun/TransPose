{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 11에서 발생하는 버그 수정\n",
    "import inspect\n",
    "\n",
    "if not hasattr(inspect, 'getargspec'):\n",
    "    inspect.getargspec = inspect.getfullargspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(paths.weights_file))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransPoseNet(\n",
       "  (pose_s1): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=72, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=512, out_features=15, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pose_s2): RNN(\n",
       "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=87, out_features=64, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=69, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pose_s3): RNN(\n",
       "    (rnn): LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=141, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=90, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tran_b1): RNN(\n",
       "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear1): Linear(in_features=87, out_features=64, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tran_b2): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "    (linear1): Linear(in_features=141, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net import TransPoseNet\n",
    "import torch\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = TransPoseNet(num_past_frame=20, num_future_frame=5)\n",
    "model.eval()  # 반드시 eval 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor(0.) tensor([ 0.1283, -0.9559,  0.0750]) tensor([-0.1194, -0.9564,  0.0774]) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(model.rnn_state, model.current_root_y, model.last_lfoot_pos, model.last_rfoot_pos, model.last_root_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py:178: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  lfoot_pos, rfoot_pos = art.math.forward_kinematics(pose[joint_set.lower_body].unsqueeze(0),\n",
      "c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 모델이 transpose_net_241226_opset17.onnx에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# TransPoseNet에서 필요한 IMU 입력 크기를 확인하여 더미 입력 생성   예시: (1, IMU 크기)\n",
    "dummy_input = (torch.randn(1, 6 * 3), torch.randn(1, 6 * 9), torch.zeros(2, 256),torch.zeros(2, 256), torch.tensor(0.0), torch.zeros(3), torch.zeros(3), torch.zeros(3))\n",
    "\n",
    "# ONNX로 변환\n",
    "onnx_file_path = \"transpose_net_241226_opset17.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,  # PyTorch 모델\n",
    "    dummy_input,  # 예제 입력\n",
    "    onnx_file_path,  # 저장할 ONNX 파일 경로\n",
    "    export_params=True,  # 가중치를 함께 저장\n",
    "    opset_version=17,  # ONNX Opset 버전 (최소 11 이상 권장)\n",
    "    input_names=[\"acc\", \"ori\", \"h_state\", \"c_state\", \"current_root_y\", \"last_lfoot_pos\", \"last_rfoot_pos\", \"last_tran\"],  # 입력 이름\n",
    "    output_names=[\"pose\", \"tran\", \"h\", \"c\", \"root_y\", \"lfoot_pos\", \"rfoot_pos\"],  # 출력 이름\n",
    "    # dynamic_axes={\n",
    "    #     \"acc\": {0: \"batch_size\"},  # 배치 크기 동적 지원\n",
    "    #     \"ori\": {0: \"batch_size\"},\n",
    "    #     \"pose\": {0: \"batch_size\"},\n",
    "    #     \"tran\": {0: \"batch_size\"},\n",
    "    # }\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"ONNX 모델이 {onnx_file_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['acc', 'ori', 'h_state', 'c_state', 'current_root_y', 'last_lfoot_pos', 'last_rfoot_pos', 'last_tran']\n",
      "Output Names: ['pose', 'tran', 'h', 'c', 'root_y', 'lfoot_pos', 'rfoot_pos']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 실행 준비\n",
    "session = ort.InferenceSession(\"transpose_net_241226_opset17.onnx\")\n",
    "\n",
    "# 모델 입력 정보 확인\n",
    "input_names = [inp.name for inp in session.get_inputs()]\n",
    "output_names = [out.name for out in session.get_outputs()]\n",
    "print(f\"Input Names: {input_names}\")\n",
    "print(f\"Output Names: {output_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX 경량화 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxsim import simplify\n",
    "import onnx\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model = onnx.load(\"transpose_net_241226_opset17.onnx\")\n",
    "\n",
    "# Simplify 모델\n",
    "# simplified_model, check = simplify(onnx_model)\n",
    "\n",
    "# Simplify 모델, 검증 비활성화\n",
    "simplified_model, check = simplify(onnx_model, skip_fuse_bn=True, check_n=False)\n",
    "\n",
    "# 단순화 후 저장\n",
    "onnx.save(simplified_model, \"simplified_model_241226_opset17.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch 모델 (실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\npython value of type 'int' cannot be used as a value. Perhaps it is a closed over global variable? If so, please consider passing it in as an argument or use a local varible instead.:\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\utils.py\", line 13\n    glb_acc = glb_acc.view(-1, 6, 3)\n    glb_ori = glb_ori.view(-1, 6, 3, 3)\n    acc = torch.cat((glb_acc[:, :5] - glb_acc[:, 5:], glb_acc[:, 5:]), dim=1).bmm(glb_ori[:, -1]) / acc_scale\n                                                                                                    ~~~~~~~~~ <--- HERE\n    ori = torch.cat((glb_ori[:, 5:].transpose(2, 3).matmul(glb_ori[:, :5]), glb_ori[:, 5:]), dim=1)\n    data = torch.cat((acc.flatten(1), ori.flatten(1)), dim=1)\n'normalize_and_concat' is being compiled since it was called from 'TransPoseNet.forward'\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py\", line 220\n    def forward(self, acc, ori):\n        data_nn = normalize_and_concat(acc, ori)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        pose, tran = self.forward_online(data_nn)\n        # pose = rotation_matrix_to_axis_angle(pose.view(1, 216)).view(72)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dummy Input: IMU 데이터 (예: [batch_size, n_imu])\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# dummy_input = torch.randn(1, 69)  # Replace 69 with your model's input size\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# TorchScript 변환\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m scripted_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m      8\u001b[0m scripted_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose_net.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1429\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1428\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1438\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1147\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1146\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:557\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    556\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:634\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:466\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    463\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m    464\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 466\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_recursive.py:1002\u001b[0m, in \u001b[0;36mtry_compile_fn\u001b[1;34m(fn, loc)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# We don't have the actual scope where the function was defined, but we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# extract the necessary info from the closed over variables on the function\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# object\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m rcb \u001b[38;5;241m=\u001b[39m _jit_internal\u001b[38;5;241m.\u001b[39mcreateResolutionCallbackFromClosure(fn)\n\u001b[1;32m-> 1002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1429\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1428\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1438\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32mc:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:1205\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rcb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1204\u001b[0m     _rcb \u001b[38;5;241m=\u001b[39m _jit_internal\u001b[38;5;241m.\u001b[39mcreateResolutionCallbackFromClosure(obj)\n\u001b[1;32m-> 1205\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_script_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqualified_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_default_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# Forward docstrings\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \npython value of type 'int' cannot be used as a value. Perhaps it is a closed over global variable? If so, please consider passing it in as an argument or use a local varible instead.:\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\utils.py\", line 13\n    glb_acc = glb_acc.view(-1, 6, 3)\n    glb_ori = glb_ori.view(-1, 6, 3, 3)\n    acc = torch.cat((glb_acc[:, :5] - glb_acc[:, 5:], glb_acc[:, 5:]), dim=1).bmm(glb_ori[:, -1]) / acc_scale\n                                                                                                    ~~~~~~~~~ <--- HERE\n    ori = torch.cat((glb_ori[:, 5:].transpose(2, 3).matmul(glb_ori[:, :5]), glb_ori[:, 5:]), dim=1)\n    data = torch.cat((acc.flatten(1), ori.flatten(1)), dim=1)\n'normalize_and_concat' is being compiled since it was called from 'TransPoseNet.forward'\n  File \"c:\\Users\\user\\dev\\transpose_livestream\\model\\TransPose\\net.py\", line 220\n    def forward(self, acc, ori):\n        data_nn = normalize_and_concat(acc, ori)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        pose, tran = self.forward_online(data_nn)\n        # pose = rotation_matrix_to_axis_angle(pose.view(1, 216)).view(72)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Input: IMU 데이터 (예: [batch_size, n_imu])\n",
    "# dummy_input = torch.randn(1, 69)  # Replace 69 with your model's input size\n",
    "\n",
    "# TorchScript 변환\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# 모델 저장\n",
    "scripted_model.save(\"transpose_net.pt\")\n",
    "print(\"TorchScript 모델이 'transpose_net.pt'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "quantized_model = quantize_dynamic(model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"quantized_transpose_net.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
